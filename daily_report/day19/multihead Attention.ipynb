{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increased-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ideal-scheme",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 128266.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pad_id = 0\n",
    "vocab_size = 100\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
    "  [75, 51],\n",
    "  [66, 88, 98, 47],\n",
    "  [21, 39, 10, 64, 21],\n",
    "  [98],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
    "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
    "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
    "]\n",
    "\n",
    "def padding(data):\n",
    "    max_len = len(max(data, key=len))\n",
    "    print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "    for i, seq in enumerate(tqdm(data)):\n",
    "        if len(seq) < max_len:\n",
    "            data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "    return data, max_len\n",
    "\n",
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-haiti",
   "metadata": {},
   "source": [
    "### Hyper parameter setting embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "great-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "batch = torch.LongTensor(data)\n",
    "batch_emb = embedding(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominican-hanging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "q,k,v = w_q(batch_emb), w_k(batch_emb), w_v(batch_emb)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceramic-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q = q.view( batch_size, -1, num_heads, d_k )\n",
    "k = k.view( batch_size, -1, num_heads, d_k )\n",
    "v = v.view( batch_size, -1, num_heads, d_k )\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "brief-tribune",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0302, 0.0287, 0.2438,  ..., 0.0294, 0.0294, 0.0294],\n",
      "         [0.0433, 0.0731, 0.0598,  ..., 0.0109, 0.0109, 0.0109],\n",
      "         [0.0843, 0.1672, 0.0237,  ..., 0.0216, 0.0216, 0.0216],\n",
      "         ...,\n",
      "         [0.0255, 0.1751, 0.1248,  ..., 0.0100, 0.0100, 0.0100],\n",
      "         [0.0255, 0.1751, 0.1248,  ..., 0.0100, 0.0100, 0.0100],\n",
      "         [0.0255, 0.1751, 0.1248,  ..., 0.0100, 0.0100, 0.0100]],\n",
      "\n",
      "        [[0.0418, 0.0156, 0.0117,  ..., 0.0587, 0.0587, 0.0587],\n",
      "         [0.0196, 0.0961, 0.0136,  ..., 0.0517, 0.0517, 0.0517],\n",
      "         [0.0103, 0.0255, 0.1457,  ..., 0.0477, 0.0477, 0.0477],\n",
      "         ...,\n",
      "         [0.1555, 0.0249, 0.1721,  ..., 0.0307, 0.0307, 0.0307],\n",
      "         [0.1555, 0.0249, 0.1721,  ..., 0.0307, 0.0307, 0.0307],\n",
      "         [0.1555, 0.0249, 0.1721,  ..., 0.0307, 0.0307, 0.0307]],\n",
      "\n",
      "        [[0.0275, 0.0317, 0.0618,  ..., 0.0801, 0.0801, 0.0801],\n",
      "         [0.0656, 0.0071, 0.0175,  ..., 0.0688, 0.0688, 0.0688],\n",
      "         [0.0924, 0.1049, 0.0383,  ..., 0.0283, 0.0283, 0.0283],\n",
      "         ...,\n",
      "         [0.1535, 0.0236, 0.0671,  ..., 0.0212, 0.0212, 0.0212],\n",
      "         [0.1535, 0.0236, 0.0671,  ..., 0.0212, 0.0212, 0.0212],\n",
      "         [0.1535, 0.0236, 0.0671,  ..., 0.0212, 0.0212, 0.0212]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0569, 0.0083, 0.0065,  ..., 0.0477, 0.0286, 0.0360],\n",
      "         [0.0413, 0.0425, 0.0973,  ..., 0.0205, 0.0218, 0.0103],\n",
      "         [0.0507, 0.0298, 0.1003,  ..., 0.0036, 0.0296, 0.0567],\n",
      "         ...,\n",
      "         [0.0370, 0.3638, 0.0292,  ..., 0.0323, 0.0166, 0.0385],\n",
      "         [0.0176, 0.0246, 0.0564,  ..., 0.0168, 0.0246, 0.0153],\n",
      "         [0.0329, 0.0494, 0.1059,  ..., 0.0281, 0.0531, 0.0761]],\n",
      "\n",
      "        [[0.0252, 0.0480, 0.0363,  ..., 0.0402, 0.0402, 0.0402],\n",
      "         [0.0467, 0.0283, 0.0447,  ..., 0.0486, 0.0486, 0.0486],\n",
      "         [0.0337, 0.0088, 0.1449,  ..., 0.0533, 0.0533, 0.0533],\n",
      "         ...,\n",
      "         [0.0971, 0.0280, 0.1251,  ..., 0.0131, 0.0131, 0.0131],\n",
      "         [0.0971, 0.0280, 0.1251,  ..., 0.0131, 0.0131, 0.0131],\n",
      "         [0.0971, 0.0280, 0.1251,  ..., 0.0131, 0.0131, 0.0131]],\n",
      "\n",
      "        [[0.0122, 0.1561, 0.2637,  ..., 0.0064, 0.0064, 0.0064],\n",
      "         [0.0996, 0.0322, 0.0086,  ..., 0.0554, 0.0554, 0.0554],\n",
      "         [0.0513, 0.0114, 0.0363,  ..., 0.0641, 0.0641, 0.0641],\n",
      "         ...,\n",
      "         [0.0856, 0.0392, 0.0703,  ..., 0.0184, 0.0184, 0.0184],\n",
      "         [0.0856, 0.0392, 0.0703,  ..., 0.0184, 0.0184, 0.0184],\n",
      "         [0.0856, 0.0392, 0.0703,  ..., 0.0184, 0.0184, 0.0184]]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([10, 20, 20])\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "attn_dists = F.softmax(attn_scores, dim = -1)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)\n",
    "\n",
    "attn_values = torch.matmul(attn_dists, v)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-rotation",
   "metadata": {},
   "source": [
    "## 각 head와 결과물 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "consolidated-usage",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n",
      "tensor([[[-0.0024, -0.0981,  0.2376,  ..., -0.0716, -0.0008, -0.2553],\n",
      "         [ 0.0360,  0.0455,  0.1208,  ...,  0.1956, -0.1243, -0.0389],\n",
      "         [-0.0028,  0.0939, -0.1629,  ..., -0.1600,  0.1667, -0.3306],\n",
      "         ...,\n",
      "         [-0.1684, -0.1064,  0.2197,  ..., -0.1322,  0.0044, -0.1180],\n",
      "         [-0.0708,  0.2256, -0.1024,  ..., -0.0662, -0.0648,  0.3130],\n",
      "         [-0.0147, -0.0364,  0.0042,  ..., -0.0326, -0.1142, -0.2119]],\n",
      "\n",
      "        [[ 0.1454, -0.2093,  0.0136,  ...,  0.2104, -0.0654, -0.3726],\n",
      "         [ 0.0288,  0.4116,  0.5411,  ...,  0.1261,  0.3474,  0.0799],\n",
      "         [-0.1704,  0.0796, -0.1691,  ..., -0.2964,  0.0923, -0.1506],\n",
      "         ...,\n",
      "         [-0.4135, -0.0215,  0.4416,  ...,  0.1586,  0.1487,  0.0887],\n",
      "         [-0.1519,  0.2000,  0.0781,  ..., -0.2038,  0.1383,  0.3918],\n",
      "         [-0.1004, -0.0034,  0.4002,  ..., -0.1959, -0.0601, -0.0217]],\n",
      "\n",
      "        [[ 0.0098, -0.2113,  0.0832,  ...,  0.1265,  0.0413, -0.3621],\n",
      "         [-0.0368,  0.1239,  0.2573,  ..., -0.1615,  0.1877,  0.0299],\n",
      "         [-0.1649,  0.1385, -0.0775,  ..., -0.0033,  0.1782, -0.1548],\n",
      "         ...,\n",
      "         [-0.1011, -0.1794,  0.2526,  ...,  0.1734, -0.1149,  0.0655],\n",
      "         [-0.1295,  0.1100,  0.1124,  ..., -0.0923,  0.1443,  0.0502],\n",
      "         [-0.1627,  0.0110,  0.4257,  ..., -0.0715, -0.0163,  0.0886]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1463,  0.0758,  0.0014,  ..., -0.0957, -0.0582,  0.0244],\n",
      "         [ 0.0547,  0.1272, -0.0786,  ..., -0.2086,  0.1353, -0.0449],\n",
      "         [ 0.0773,  0.0765,  0.1656,  ..., -0.0701,  0.0349, -0.0616],\n",
      "         ...,\n",
      "         [ 0.1178,  0.2053, -0.0300,  ...,  0.0718, -0.0883, -0.0061],\n",
      "         [ 0.0321,  0.0228, -0.0029,  ...,  0.1587, -0.0251, -0.0794],\n",
      "         [ 0.0460,  0.0727,  0.1405,  ...,  0.0483,  0.1832,  0.0094]],\n",
      "\n",
      "        [[-0.0520, -0.0860,  0.3191,  ..., -0.0828,  0.2025, -0.1627],\n",
      "         [ 0.0394,  0.3340,  0.0487,  ..., -0.0836,  0.0990, -0.2273],\n",
      "         [ 0.0218,  0.1136, -0.0269,  ...,  0.0805,  0.0042,  0.0155],\n",
      "         ...,\n",
      "         [-0.0967, -0.0373,  0.1686,  ...,  0.0715,  0.1042,  0.0641],\n",
      "         [ 0.2453,  0.1345,  0.0235,  ..., -0.0832,  0.0801,  0.0476],\n",
      "         [ 0.0263, -0.0971,  0.2509,  ...,  0.0825,  0.0297, -0.0745]],\n",
      "\n",
      "        [[-0.0489, -0.0973,  0.0835,  ..., -0.1328,  0.2263, -0.1659],\n",
      "         [-0.1405,  0.1231,  0.1636,  ..., -0.0846,  0.3663,  0.2149],\n",
      "         [-0.2974,  0.1926,  0.1119,  ..., -0.1692,  0.0334, -0.3097],\n",
      "         ...,\n",
      "         [ 0.0494,  0.0990,  0.0317,  ..., -0.0116, -0.1026, -0.0340],\n",
      "         [ 0.0804,  0.3265,  0.0209,  ...,  0.0466,  0.0802, -0.0043],\n",
      "         [-0.1791, -0.0271,  0.0510,  ...,  0.0423, -0.1319,  0.0993]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "attn_values = attn_values.transpose(1,2)\n",
    "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)\n",
    "\n",
    "print(attn_values.shape)\n",
    "\n",
    "outputs = w_0(attn_values)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stainless-fairy",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3],\n",
       "        [4, 4, 4],\n",
       "        [5, 5, 5],\n",
       "        [6, 6, 6],\n",
       "        [7, 7, 7],\n",
       "        [8, 8, 8],\n",
       "        [9, 9, 9]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        # Q, K, V learnable matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Linear transformation for concatenated outputs\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.w_q(q)  # (B, L, d_model)\n",
    "        k = self.w_k(k)  # (B, L, d_model)\n",
    "        v = self.w_v(v)  # (B, L, d_model)\n",
    "\n",
    "        q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "        k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "        v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "        k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "        v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "        attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
    "        attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
    "\n",
    "        return self.w_0(attn_values)\n",
    "\n",
    "    def self_attention(self, q, k, v):\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "        attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "        attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "        return attn_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_3.7] *",
   "language": "python",
   "name": "conda-env-python_3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
