{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "  \"정말 맛있습니다. 추천합니다.\",\n",
    "  \"기대했던 것보단 별로였네요.\",\n",
    "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
    "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
    "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
    "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
    "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
    "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
    "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
    "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \n",
    "]\n",
    "\n",
    "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized(data):\n",
    "    tokenized = []\n",
    "    for sent in tqdm(data):\n",
    "        tokens = tokenizer.morphs(sent, stem = True)\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = make_tokenized(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 73326.99it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "\n",
    "for tokens in tqdm(train_tokenized):\n",
    "    for token in tokens:\n",
    "        word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 2762.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2i = {} \n",
    "for pair in tqdm(word_count):\n",
    "    if pair[0] not in w2i:\n",
    "        w2i[pair[0]] = len(w2i)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, train_tokenized, window_size = 2):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        \n",
    "        for tokens in tqdm(train_tokenized):\n",
    "            token_ids = [w2i[token] for token in tokens]\n",
    "            for i, id in enumerate(token_ids):\n",
    "                if i-window_size >= 0 and i+window_size < len(token_ids):\n",
    "                    self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
    "                    self.y.append(id)\n",
    "        self.x = torch.LongTensor(self.x)\n",
    "        self.y = torch.LongTensor(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, train_tokenized, window_size = 2 ):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        \n",
    "        for tokens in tqdm(train_tokenized):\n",
    "            token_ids = [w2i[token] for token in tokens]\n",
    "            for i, id in enumerate(token_ids):\n",
    "                if i-window_size >= 0 and i+window_size < len(token_ids):\n",
    "                    self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
    "                    self.x += [id] * 2 * window_size\n",
    "        self.x = torch.LongTensor(self.x)\n",
    "        self.y = torch.LongTensor(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11134.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 56223.91it/s]\n"
     ]
    }
   ],
   "source": [
    "cbow_set = CBOWDataset(train_tokenized)\n",
    "skipgram_set = SkipGramDataset(train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
    "        self.linear = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = torch.sum(embeddings, dim=1)\n",
    "        output = self.linear(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
    "        self.linear = nn.Linear(dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        output = self.linear(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = CBOW(vocab_size=len(w2i), dim=256)\n",
    "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
    "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 199.79it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1025.77it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 849.12it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 858.51it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 389.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Epoch: 1\n",
      "Train loss: 4.06065559387207\n",
      "Train loss: 3.2805368900299072\n",
      "Train loss: 5.238283157348633\n",
      "Train loss: 5.654989242553711\n",
      "Train loss: 4.657295227050781\n",
      "Train loss: 5.284740447998047\n",
      "Train loss: 6.04366397857666\n",
      "Train loss: 3.4834835529327393\n",
      "Train loss: 4.7581586837768555\n",
      "Train loss: 4.869666576385498\n",
      "Train loss: 4.379453659057617\n",
      "Train loss: 4.701166152954102\n",
      "Train loss: 3.7360734939575195\n",
      "Train loss: 5.297375679016113\n",
      "Train loss: 4.67045783996582\n",
      "Train loss: 4.100661277770996\n",
      "##################################################\n",
      "Epoch: 2\n",
      "Train loss: 3.9094510078430176\n",
      "Train loss: 3.1645917892456055\n",
      "Train loss: 5.103442192077637\n",
      "Train loss: 5.521780967712402\n",
      "Train loss: 4.538938999176025\n",
      "Train loss: 4.990396022796631\n",
      "Train loss: 5.808369159698486\n",
      "Train loss: 3.3620362281799316\n",
      "Train loss: 4.640562057495117\n",
      "Train loss: 4.712558269500732\n",
      "Train loss: 4.196480751037598\n",
      "Train loss: 4.322608470916748\n",
      "Train loss: 3.595561981201172\n",
      "Train loss: 5.162219524383545\n",
      "Train loss: 4.516561031341553\n",
      "Train loss: 3.956862211227417\n",
      "##################################################\n",
      "Epoch: 3\n",
      "Train loss: 3.7608542442321777\n",
      "Train loss: 3.0523312091827393\n",
      "Train loss: 4.970224857330322\n",
      "Train loss: 5.389398574829102\n",
      "Train loss: 4.422131538391113\n",
      "Train loss: 4.7052001953125\n",
      "Train loss: 5.582712173461914\n",
      "Train loss: 3.2453689575195312\n",
      "Train loss: 4.52569580078125\n",
      "Train loss: 4.558757305145264\n",
      "Train loss: 4.021188735961914\n",
      "Train loss: 3.9605464935302734\n",
      "Train loss: 3.458580732345581\n",
      "Train loss: 5.029200553894043\n",
      "Train loss: 4.364965915679932\n",
      "Train loss: 3.816774368286133\n",
      "##################################################\n",
      "Epoch: 4\n",
      "Train loss: 3.614975929260254\n",
      "Train loss: 2.943739414215088\n",
      "Train loss: 4.8386149406433105\n",
      "Train loss: 5.2578535079956055\n",
      "Train loss: 4.306890964508057\n",
      "Train loss: 4.430839538574219\n",
      "Train loss: 5.3654985427856445\n",
      "Train loss: 3.13307523727417\n",
      "Train loss: 4.413618087768555\n",
      "Train loss: 4.4086222648620605\n",
      "Train loss: 3.854811191558838\n",
      "Train loss: 3.6184117794036865\n",
      "Train loss: 3.325054407119751\n",
      "Train loss: 4.898305892944336\n",
      "Train loss: 4.215746879577637\n",
      "Train loss: 3.6804165840148926\n",
      "##################################################\n",
      "Epoch: 5\n",
      "Train loss: 3.4719393253326416\n",
      "Train loss: 2.8387904167175293\n",
      "Train loss: 4.7086076736450195\n",
      "Train loss: 5.127164840698242\n",
      "Train loss: 4.193235874176025\n",
      "Train loss: 4.169252395629883\n",
      "Train loss: 5.155735015869141\n",
      "Train loss: 3.024846076965332\n",
      "Train loss: 4.304382801055908\n",
      "Train loss: 4.262537479400635\n",
      "Train loss: 3.698655605316162\n",
      "Train loss: 3.299938440322876\n",
      "Train loss: 3.194913625717163\n",
      "Train loss: 4.769542694091797\n",
      "Train loss: 4.069000720977783\n",
      "Train loss: 3.54781436920166\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cbow.train()\n",
    "cbow = cbow.to(device)\n",
    "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    print(\"#\" * 50)\n",
    "    print(f\"Epoch: {e}\")\n",
    "    for batch in tqdm(cbow_loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
    "        output = cbow(x)  # (B, V)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optim.step() \n",
    "\n",
    "        print(f\"Train loss: {loss.item()}\")\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.train()\n",
    "skipgram = skipgram.to(device)\n",
    "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    print(\"#\" * 50)\n",
    "    print(f\"Epoch: {e}\")\n",
    "    for batch in tqdm(skipgram_loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
    "        output = skipgram(x)  # (B, V)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print(f\"Train loss: {loss.item()}\")\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in test_words:\n",
    "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
    "    emb = cbow.embedding(input_id)\n",
    "\n",
    "    print(f\"Word: {word}\")\n",
    "    print(emb.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in test_words:\n",
    "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
    "    emb = skipgram.embedding(input_id)\n",
    "\n",
    "    print(f\"Word: {word}\")\n",
    "    print(max(emb.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 21,  4,  2])\n",
      "tensor(22)\n",
      "torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5033, -0.6327,  1.9857,  1.4902], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = list(cbow_set)[2]\n",
    "print(x)\n",
    "print(y)\n",
    "embedding = nn.Embedding(num_embeddings=60 , embedding_dim=10, sparse=True)\n",
    "a = embedding(x)\n",
    "print(a.shape)\n",
    "torch.sum(a, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(19)\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x,y = list(skipgram_set)[2]\n",
    "print(x)\n",
    "print(y)\n",
    "embedding = nn.Embedding(num_embeddings=60 , embedding_dim=10, sparse=True)\n",
    "a = embedding(x)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 256)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbow_set), len(skipgram_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9207],\n",
       "        [ 0.0544],\n",
       "        [ 1.0189],\n",
       "        [-0.0885],\n",
       "        [ 0.5778],\n",
       "        [ 0.1303],\n",
       "        [ 1.1415],\n",
       "        [ 0.4251],\n",
       "        [ 1.6866],\n",
       "        [-0.8761]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9207,  0.0544,  1.0189, -0.0885,  0.5778,  0.1303,  1.1415,  0.4251,\n",
       "         1.6866, -0.8761], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
