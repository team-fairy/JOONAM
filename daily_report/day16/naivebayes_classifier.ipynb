{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from konlpy import tag # 다양한 한국어 형태소 분석기 클래스가 담겨있다\n",
    "from collections import defaultdict\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "  \"정말 맛있습니다. 추천합니다.\",\n",
    "  \"기대했던 것보단 별로였네요.\",\n",
    "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
    "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
    "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
    "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
    "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
    "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
    "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
    "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"\n",
    "]\n",
    "train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "\n",
    "test_data = [\n",
    "  \"정말 좋았습니다. 또 가고 싶네요.\",\n",
    "  \"별로였습니다. 되도록 가지 마세요.\",\n",
    "  \"다른 분들께도 추천드릴 수 있을 만큼 만족했습니다.\",\n",
    "  \"서비스가 좀 더 개선되었으면 좋겠습니다. 기분이 좀 나빴습니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tag.Okt()\n",
    "\n",
    "def make_tokenized(data):\n",
    "    tokenized = []\n",
    "    for sent in tqdm(data):\n",
    "        tokens = tokenizer.morphs(sent)\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 151.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['정말', '맛있습니다', '.', '추천', '합니다', '.'], ['기대했던', '것', '보단', '별로', '였네요', '.'], ['다', '좋은데', '가격', '이', '너무', '비싸서', '다시', '가고', '싶다는', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '입니다', '!', '재', '방문', '의사', '있습니다', '.'], ['음식', '도', '서비스', '도', '다', '만족스러웠습니다', '.'], ['위생', '상태', '가', '좀', '별로', '였습니다', '.', '좀', '더', '개선', '되', '기를', '바랍니다', '.'], ['맛', '도', '좋았고', '직원', '분들', '서비스', '도', '너무', '친절했습니다', '.'], ['기념일', '에', '방문', '했는데', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋았습니다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짰습니다', '.', '저', '는', '별로', '였네요', '.'], ['위생', '에', '조금', '더', '신경', '썼으면', '좋겠습니다', '.', '조금', '불쾌했습니다', '.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = make_tokenized( train_data )\n",
    "test_tokenized = make_tokenized( test_data )\n",
    "print(train_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 66260.73it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int) # collections 활용\n",
    "\n",
    "for tokens in tqdm(train_tokenized):\n",
    "    for token in tokens:\n",
    "        word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "word_count = sorted( word_count.items(), key = lambda x : x[1], reverse = True)\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어, token별 idx부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:00<00:00, 133602.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, '도': 1, '별로': 2, '다': 3, '이': 4, '너무': 5, '음식': 6, '서비스': 7, '였네요': 8, '방문': 9, '위생': 10, '좀': 11, '더': 12, '에': 13, '조금': 14, '정말': 15, '맛있습니다': 16, '추천': 17, '합니다': 18, '기대했던': 19, '것': 20, '보단': 21, '좋은데': 22, '가격': 23, '비싸서': 24, '다시': 25, '가고': 26, '싶다는': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '입니다': 34, '!': 35, '재': 36, '의사': 37, '있습니다': 38, '만족스러웠습니다': 39, '상태': 40, '가': 41, '였습니다': 42, '개선': 43, '되': 44, '기를': 45, '바랍니다': 46, '맛': 47, '좋았고': 48, '직원': 49, '분들': 50, '친절했습니다': 51, '기념일': 52, '했는데': 53, '분위기': 54, '좋았습니다': 55, '전반': 56, '적': 57, '으로': 58, '짰습니다': 59, '저': 60, '는': 61, '신경': 62, '썼으면': 63, '좋겠습니다': 64, '불쾌했습니다': 65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2i = {}\n",
    "for pair in tqdm(word_count):\n",
    "    if pair[0] not in w2i:\n",
    "        w2i[pair[0]] = len(w2i)\n",
    "print(w2i) # 단어마다 개별 index를 부어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "\n",
    "## Naive Bayes 구현하기\n",
    "\n",
    "- self.k : Smoothing을 위한 상수.\n",
    "- self.w2i : 사전에 구한 vocab\n",
    "- self.priors : 각 class 의 사전 확률\n",
    "- self.likelihoods : 각 token의 특정 조건 내의 likelihood..?\n",
    "\n",
    "- smoothing을 위해서 라플라스 스무딩 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, w2i, k=0.1):\n",
    "        self.k = k\n",
    "        self.w2i = w2i\n",
    "        self.priors = {}\n",
    "        self.likelihoods = {}\n",
    "        \n",
    "    def train( self, train_tokenized, train_labels ):\n",
    "        self.set_priors(train_labels) \n",
    "        self.set_likelihoods(train_tokenized, train_labels)\n",
    "        \n",
    "    def inference(self, tokens):\n",
    "        log_prob0 = 0.0\n",
    "        log_prob1 = 0.0\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.likelihoods:\n",
    "                log_prob0 += math.log(self.likelihoods[token][0])\n",
    "                log_prob1 += math.log(self.likelihoods[token][1])\n",
    "        \n",
    "        log_prob0 += math.log(self.priors[0])\n",
    "        log_prob1 += math.log(self.priors[1])\n",
    "        \n",
    "        \n",
    "        if log_prob0 >= log_prob1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def set_priors(self, train_labels):\n",
    "        class_counts = defaultdict(int)\n",
    "        \n",
    "        for label in tqdm(train_labels):\n",
    "            class_counts[label] += 1\n",
    "        for label, count in class_counts.items():\n",
    "            self.priors[label] = class_counts[label] / len(train_labels)\n",
    "            \n",
    "            \n",
    "    def set_likelihoods(self, train_tokenized, train_labels):\n",
    "        token_dists = {}\n",
    "        class_counts = defaultdict(int)\n",
    "        \n",
    "        for i, label in enumerate(tqdm(train_labels)):\n",
    "            count = 0\n",
    "            for token in train_tokenized[i]:\n",
    "                if token in self.w2i:\n",
    "                    if token not in token_dists:\n",
    "                        token_dists[token] = {0:0,1:0}\n",
    "                    token_dists[token][label] += 1\n",
    "                    count += 1\n",
    "            class_counts[label] += count\n",
    "        \n",
    "        for token,dist in tqdm(token_dists.items()):\n",
    "            if token not in self.likelihoods:\n",
    "                self.likelihoods[token] = {\n",
    "                    0:(token_dists[token][0] + self.k) / (class_counts[0] + len(self.w2i)*self.k),\n",
    "                    1:(token_dists[token][1] + self.k) / (class_counts[1] + len(self.w2i)*self.k),\n",
    "                }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 101803.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 69098.91it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 399457.52it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier(w2i)\n",
    "classifier.train(train_tokenized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 30727.50it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for test_tokens in tqdm(test_tokenized):\n",
    "    pred = classifier.inference(test_tokens)\n",
    "    preds.append(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
