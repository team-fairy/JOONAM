{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFLiGyKDKUmF"
   },
   "source": [
    "# 실습 10. \n",
    "\n",
    "**from dgl.nn import SAGEConv** 를 직접 구현하고, 이를 이용하여 GraphSAGE 모델을 학습시켜보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Lp2Pmzp9MThS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dgl\n",
      "  Downloading dgl-0.6.0-cp37-cp37m-macosx_10_9_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from dgl) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from dgl) (1.19.5)\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from dgl) (2.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from dgl) (1.6.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from networkx>=2.1->dgl) (4.4.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/envs/python_3.7/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (4.0.0)\n",
      "Installing collected packages: dgl\n",
      "Successfully installed dgl-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-2wYGnS9MUOa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                        \n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from sklearn.metrics import f1_score\n",
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4EWxJtfMM6a"
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 정의\n",
    "learningRate = 1e-2\n",
    "numEpochs = 50\n",
    "numHiddenDim = 128\n",
    "numLayers = 2\n",
    "weightDecay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfKIrNuqRpVa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Cora 데이터셋은 2708개의 논문(노드), 10556개의 인용관계(엣지)로 이루어졌습니다. \n",
    "    NumFeat은 각 노드를 나타내는 특성을 말합니다. \n",
    "    Cora 데이터셋은 각 노드가 1433개의 특성을 가지고, 개개의 특성은 '1'혹은 '0'으로 나타내어지며 특정 단어의 논문 등장 여부를 나타냅니다.\n",
    "    즉, 2708개의 논문에서 특정 단어 1433개를 뽑아서, 1433개의 단어의 등장 여부를 통해 각 노드를 표현합니다.\n",
    "    \n",
    "    노드의 라벨은 총 7개가 존재하고, 각 라벨은 논문의 주제를 나타냅니다\n",
    "    [Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory]\n",
    "\n",
    "    2708개의 노드 중, 학습에는 140개의 노드를 사용하고 모델을 테스트하는 데에는 1000개를 사용합니다.\n",
    "    본 실습에서는 Validation을 진행하지않습니다.\n",
    "\n",
    "    요약하자면, 앞서 학습시킬 모델은 Cora 데이터셋의 \n",
    "    [논문 내 등장 단어들, 논문들 사이의 인용관계]를 활용하여 논문의 주제를 예측하는 모델입니다.\n",
    "'''\n",
    "\n",
    "# Cora Graph Dataset 불러오기\n",
    "G = CoraGraphDataset()\n",
    "numClasses = G.num_classes\n",
    "\n",
    "G = G[0]\n",
    "# 노드들의 feauture & feature의 차원\n",
    "features = G.ndata['feat']\n",
    "inputFeatureDim = features.shape[1]\n",
    "\n",
    "# 각 노드들의 실제 라벨\n",
    "labels = G.ndata['label']\n",
    "\n",
    "# 학습/테스트에 사용할 노드들에 대한 표시\n",
    "trainMask = G.ndata['train_mask']        \n",
    "testMask = G.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNlgAxzmMx4q"
   },
   "outputs": [],
   "source": [
    "# 모델 학습 결과를 평가할 함수\n",
    "def evaluateTrain(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "def evaluateTest(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        macro_f1 = f1_score(labels, indices, average = 'macro')\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels), macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Px-uArTVMztb"
   },
   "outputs": [],
   "source": [
    "def train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs):\n",
    "    executionTime=[]\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        model.train()\n",
    "\n",
    "        startTime = time.time()\n",
    "            \n",
    "        logits = model(features)                                    # 포워딩\n",
    "        loss = lossFunction(logits[trainMask], labels[trainMask])   # 모델의 예측값과 실제 라벨을 비교하여 loss 값 계산\n",
    "\n",
    "        optimizer.zero_grad()                                       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        executionTime.append(time.time() - startTime)\n",
    "\n",
    "        acc = evaluateTrain(model, features, labels, trainMask)\n",
    "\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f}\".format(epoch, np.mean(executionTime), loss.item(), acc))\n",
    "\n",
    "def test(model, feautures, labels, testMask):\n",
    "    acc, macro_f1 = evaluateTest(model, features, labels, testMask)\n",
    "    print(\"Test Accuracy {:.4f}\".format(acc))\n",
    "    print(\"Test macro-f1 {:.4f}\".format(macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvFtl2-jIfiW"
   },
   "outputs": [],
   "source": [
    "class SAGEConv(nn.Module):\n",
    "    \"\"\"\n",
    "    in_feats: 인풋 feature의 사이즈\n",
    "    out_feats: 아웃풋 feature의 사이즈\n",
    "    activation: None이 아니라면, 노드 피쳐의 업데이트를 위해서 해당 activation function을 적용한다.\n",
    "    \"\"\"\n",
    "    '''\n",
    "        ref:\n",
    "        https://arxiv.org/pdf/1706.02216.pdf \n",
    "        https://docs.dgl.ai/en/0.4.x/_modules/dgl/nn/pytorch/conv/sageconv.html\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self.activation = activation\n",
    "\n",
    "        self.W = nn.Linear(in_feats+in_feats, out_feats, bias=True)\n",
    "\n",
    "    def forward(self, graph, feature):\n",
    "        graph.ndata['h'] = '''Insert Here'''                                                      \n",
    "        graph.'''Insert Here'''                   \n",
    "\n",
    "        # Aggregate & Noramlization\n",
    "        degs = '''Insert Here'''\n",
    "        hkNeigh = '''Insert Here'''\n",
    "        \n",
    "        hk = '''Insert Here'''                \n",
    "\n",
    "        if self.activation != None:\n",
    "            hk = self.activation(hk)\n",
    "\n",
    "        return hk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7_VGTIZMUQp"
   },
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    '''\n",
    "        graph               : 학습할 그래프\n",
    "        inFeatDim           : 데이터의 feature의 차원\n",
    "        numHiddenDim        : 모델의 hidden 차원\n",
    "        numClasses          : 예측할 라벨의 경우의 수\n",
    "        numLayers           : 인풋, 아웃풋 레이어를 제외하고 중간 레이어의 갯수\n",
    "        activationFunction  : 활성화 함수의 종류\n",
    "    '''\n",
    "    def __init__(self, graph, inFeatDim, numHiddenDim, numClasses, numLayers, activationFunction):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.graph = graph\n",
    "\n",
    "        # 인풋 레이어\n",
    "        self.layers.append(SAGEConv(inFeatDim, numHiddenDim, activationFunction))\n",
    "       \n",
    "        # 히든 레이어\n",
    "        for i in range(numLayers):\n",
    "            self.layers.append(SAGEConv(numHiddenDim, numHiddenDim, activationFunction))\n",
    "        \n",
    "        # 출력 레이어\n",
    "        self.layers.append(SAGEConv(numHiddenDim, numClasses, activation=None))\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features\n",
    "        for layer in self.layers:\n",
    "            x = layer(self.graph, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RSfUBNJN_4L"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = GraphSAGE(G, inputFeatureDim, numHiddenDim, numClasses, numLayers, F.relu)\n",
    "print(model)\n",
    "\n",
    "lossFunction = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 초기화\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0i0GwVwMUSy"
   },
   "outputs": [],
   "source": [
    "train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ng-_-vQCMUUa"
   },
   "outputs": [],
   "source": [
    "test(model, features, labels, testMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNN4yTAWJKcJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab10. using GDL Library(2)의 사본",
   "provenance": [
    {
     "file_id": "1QLXfLyo1YT-TDae2B-7DH9zS3NfOtHIJ",
     "timestamp": 1614303973038
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
