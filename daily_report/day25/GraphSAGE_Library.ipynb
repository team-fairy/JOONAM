{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab09. using GDL Library(1)의 사본","provenance":[{"file_id":"13p0xpBdtZaZlwwHGi6ZlGWtykaTdowC7","timestamp":1614300605474}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NW394yGalVGt"},"source":["# 실습 09. \r\n","\r\n","**from dgl.nn import SAGEConv** 를 활용하여 GraphSAGE 모델을 구현하고 학습시켜보기"]},{"cell_type":"code","metadata":{"id":"Pqrq01umVpvh"},"source":["!pip install dgl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2yfG0Aj_oej"},"source":["import numpy as np\r\n","import time\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import dgl\r\n","from dgl.data import CoraGraphDataset\r\n","from sklearn.metrics import f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y56IETYS-lqg"},"source":["# 하이퍼파라미터 초기화\r\n","dropoutProb = 0.5\r\n","learningRate = 1e-2\r\n","numEpochs = 50\r\n","numHiddenDim = 128\r\n","numLayers = 2\r\n","weightDecay = 5e-4\r\n","aggregatorType = \"gcn\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_ux5YI1pEh0"},"source":["'''\r\n","    Cora 데이터셋은 2708개의 논문(노드), 10556개의 인용관계(엣지)로 이루어졌습니다. \r\n","    NumFeat은 각 노드를 나타내는 특성을 말합니다. \r\n","    Cora 데이터셋은 각 노드가 1433개의 특성을 가지고, 개개의 특성은 '1'혹은 '0'으로 나타내어지며 특정 단어의 논문 등장 여부를 나타냅니다.\r\n","    즉, 2708개의 논문에서 특정 단어 1433개를 뽑아서, 1433개의 단어의 등장 여부를 통해 각 노드를 표현합니다.\r\n","    \r\n","    노드의 라벨은 총 7개가 존재하고, 각 라벨은 논문의 주제를 나타냅니다\r\n","    [Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory]\r\n","\r\n","    2708개의 노드 중, 학습에는 140개의 노드를 사용하고 모델을 테스트하는 데에는 1000개를 사용합니다.\r\n","    본 실습에서는 Validation을 진행하지않습니다.\r\n","\r\n","    요약하자면, 앞서 학습시킬 모델은 Cora 데이터셋의 \r\n","    [논문 내 등장 단어들, 논문들 사이의 인용관계]를 활용하여 논문의 주제를 예측하는 모델입니다.\r\n","'''\r\n","\r\n","# Cora Graph Dataset 불러오기\r\n","G = CoraGraphDataset()\r\n","numClasses = G.num_classes\r\n","\r\n","G = G[0]\r\n","# 노드들의 feauture & feature의 차원\r\n","features = G.ndata['feat']\r\n","inputFeatureDim = features.shape[1]\r\n","\r\n","# 각 노드들의 실제 라벨\r\n","labels = G.ndata['label']\r\n","\r\n","# 학습/테스트에 사용할 노드들에 대한 표시\r\n","trainMask = G.ndata['train_mask']        \r\n","testMask = G.ndata['test_mask']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfp4dy6TpEfl"},"source":["# 모델 학습 결과를 평가할 함수\r\n","def evaluateTrain(model, features, labels, mask):\r\n","    model.eval()\r\n","    with torch.no_grad():\r\n","        logits = model(features)\r\n","        logits = logits[mask]\r\n","        labels = labels[mask]\r\n","        _, indices = torch.max(logits, dim=1)\r\n","        correct = torch.sum(indices == labels)\r\n","        return correct.item() * 1.0 / len(labels)\r\n","\r\n","def evaluateTest(model, features, labels, mask):\r\n","    model.eval()\r\n","    with torch.no_grad():\r\n","        logits = model(features)\r\n","        logits = logits[mask]\r\n","        labels = labels[mask]\r\n","        _, indices = torch.max(logits, dim=1)\r\n","        macro_f1 = f1_score(labels, indices, average = 'macro')\r\n","        correct = torch.sum(indices == labels)\r\n","        return correct.item() * 1.0 / len(labels), macro_f1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVMJ1qDS84fI"},"source":["def train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs):\r\n","    executionTime=[]\r\n","    \r\n","    for epoch in range(numEpochs):\r\n","        model.train()\r\n","\r\n","        startTime = time.time()\r\n","            \r\n","        logits = model(features)                                    # 포워딩\r\n","        loss = lossFunction(logits[trainMask], labels[trainMask])   # 모델의 예측값과 실제 라벨을 비교하여 loss 값 계산\r\n","\r\n","        optimizer.zero_grad()                                       \r\n","        loss.backward()\r\n","        optimizer.step()\r\n","\r\n","        executionTime.append(time.time() - startTime)\r\n","\r\n","        acc = evaluateTrain(model, features, labels, trainMask)\r\n","\r\n","        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f}\".format(epoch, np.mean(executionTime), loss.item(), acc))\r\n","\r\n","def test(model, feautures, labels, testMask):\r\n","    acc, macro_f1 = evaluateTest(model, features, labels, testMask)\r\n","    print(\"Test Accuracy {:.4f}\".format(acc))\r\n","    print(\"Test macro-f1 {:.4f}\".format(macro_f1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYuLoRxfYQyY"},"source":["# Define GraphSage architecture\r\n","# 기존에 구현되어 있는 SAGEConv 모듈을 불러와서, SAGEConv로 이루어진 GraphSAGE 모델을 구축한다.\r\n","from dgl.nn.pytorch.conv import SAGEConv\r\n","class GraphSAGE(nn.Module):\r\n","    '''\r\n","        graph               : 학습할 그래프\r\n","        inFeatDim           : 데이터의 feature의 차원\r\n","        numHiddenDim        : 모델의 hidden 차원\r\n","        numClasses          : 예측할 라벨의 경우의 수\r\n","        numLayers           : 인풋, 아웃풋 레이어를 제외하고 중간 레이어의 갯수\r\n","        activationFunction  : 활성화 함수의 종류\r\n","        dropoutProb         : 드롭아웃 할 확률\r\n","        aggregatorType      : [mean, gcn, pool (for max), lstm]\r\n","    '''\r\n","    '''\r\n","        SAGEConv(inputFeatDim, outputFeatDim, aggregatorType, dropoutProb, activationFunction)와 같은 형식으로 모듈 생성\r\n","    '''\r\n","    def __init__(self,graph, inFeatDim, numHiddenDim, numClasses, numLayers, activationFunction, dropoutProb, aggregatorType):\r\n","        super(GraphSAGE, self).__init__()\r\n","        self.layers = nn.ModuleList()\r\n","        self.graph = graph\r\n","\r\n","        # 인풋 레이어\r\n","        self.layers.append(''' Insert Here! ''')\r\n","       \r\n","        # 히든 레이어\r\n","        for i in range(''' Insert Here! '''):\r\n","            self.layers.append(''' Insert Here! ''')\r\n","        \r\n","        # 출력 레이어\r\n","        self.layers.append(SAGEConv(numHiddenDim, numClasses, aggregatorType, dropoutProb, activation=None))\r\n","\r\n","    def forward(self, features):\r\n","        x = features\r\n","        for layer in self.layers:\r\n","            x = layer(self.graph, x)\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKeX9AdBpJaN"},"source":["# 모델 생성\r\n","model = GraphSAGE(G, inputFeatureDim, numHiddenDim, numClasses, numLayers, F.relu, dropoutProb, aggregatorType)\r\n","print(model)\r\n","lossFunction = torch.nn.CrossEntropyLoss()\r\n","\r\n","# 옵티마이저 초기화\r\n","optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wY9nnzs1pJcb"},"source":["train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-swaKM7E-KiY"},"source":["test(model, features, labels, testMask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5E2HkTNA6DR"},"source":[""],"execution_count":null,"outputs":[]}]}